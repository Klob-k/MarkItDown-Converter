{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": "# PDF to Markdown Converter\n## Hybrid Pipeline with Quality Detection & Claude Vision Fallback\n\nThis notebook converts PDFs to clean, well-structured Markdown using a cost-efficient hybrid approach:\n\n1. **MarkItDown** - Fast, free initial extraction\n2. **Heuristic Quality Checks** - Instant validation of extraction quality\n3. **Claude Quality Assessment** - AI-powered quality scoring\n4. **Claude Vision Fallback** - Direct PDF analysis for complex documents\n5. **Large PDF Chunking** - Automatic splitting for PDFs exceeding API limits (>100 pages or >32MB)\n\n### Claude API PDF Limits\n- **Max file size**: 32 MB per request\n- **Max pages**: 100 pages per request\n- Large PDFs are automatically chunked and results combined\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell1_header"
   },
   "source": [
    "## Cell 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "install_deps",
    "outputId": "6781b12c-d26f-4b6a-fc29-bd5db5e0c647"
   },
   "outputs": [],
   "source": "# Install with all optional dependencies\n!pip install -q 'markitdown[all]' anthropic pypdf\n\nprint(\"✓ Dependencies installed successfully\")"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ze1Ga_oFUxyI",
    "outputId": "62644df6-f3a3-4a65-d523-6544152ff8eb"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✓ pdfminer.six installed (required for MarkItDown PDF support)\n"
     ]
    }
   ],
   "source": [
    "# Verify PDF dependencies\n",
    "import subprocess\n",
    "result = subprocess.run([\"pip\", \"show\", \"pdfminer.six\"], capture_output=True, text=True)\n",
    "if \"Name: pdfminer.six\" in result.stdout:\n",
    "    print(\"✓ pdfminer.six installed (required for MarkItDown PDF support)\")\n",
    "else:\n",
    "    print(\"✗ pdfminer.six NOT found - installing directly...\")\n",
    "    !pip install -q pdfminer.six\n",
    "    print(\"✓ Installed pdfminer.six\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell2_header"
   },
   "source": [
    "## Cell 2: Configuration & Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "config",
    "outputId": "9fd3baf7-2f30-4a60-d6da-05473c42e674"
   },
   "outputs": [],
   "source": "import os\nimport base64\nimport json\nimport re\nimport tempfile\nimport time\nfrom pathlib import Path\nfrom google.colab import userdata\nfrom anthropic import Anthropic\nfrom markitdown import MarkItDown\nfrom pypdf import PdfReader, PdfWriter\n\n# =============================================================================\n# CONFIGURATION\n# =============================================================================\n\n# API Setup\nANTHROPIC_API_KEY = userdata.get('Claude_Colab')\nclient = Anthropic(api_key=ANTHROPIC_API_KEY)\n\n# Model Configuration\nMODEL = \"claude-haiku-4-5-20251001\"\nMAX_TOKENS = 16384  # Increased for longer page content\n\n# API Retry Configuration\nAPI_RETRY = {\n    \"max_retries\": 3,\n    \"base_delay\": 2,  # seconds\n    \"retryable_errors\": [\"rate\", \"overloaded\", \"timeout\", \"connection\"],\n}\n\n# Claude API PDF Limits\nPDF_LIMITS = {\n    \"max_file_size_mb\": 32,         # Maximum file size for Claude Vision\n    \"max_pages_per_request\": 100,   # Maximum pages per API request\n    \"chunk_size_pages\": 95,         # Pages per chunk (with buffer)\n    \"chunk_size_mb\": 30,            # Target chunk size in MB (with buffer)\n}\n\n# Quality Thresholds\nQUALITY_THRESHOLDS = {\n    \"min_words_per_page\": 50,       # Minimum words expected per page\n    \"max_whitespace_ratio\": 0.40,   # Maximum whitespace allowed\n    \"min_quality_score\": 7,         # Minimum Claude quality score (1-10)\n    \"skip_cleanup_score\": 9,        # Skip cleanup if score >= this\n    \"min_headers_for_long_doc\": 1,  # Minimum headers for docs > 500 words\n    \"quality_sample_chars\": 8000,   # Characters to sample for quality check\n}\n\n# File Paths\nINPUT_PDF = \"/content/input.pdf\"\nOUTPUT_MD = \"/content/output.md\"\n\n# Initialize MarkItDown\nmarkitdown = MarkItDown()\n\n\ndef api_call_with_retry(api_func, description: str = \"API call\"):\n    \"\"\"\n    Execute API call with exponential backoff retry for transient errors.\n    \n    Args:\n        api_func: Callable that makes the API request\n        description: Description for logging\n    \n    Returns:\n        API response\n    \n    Raises:\n        Exception: If all retries exhausted or non-retryable error\n    \"\"\"\n    for attempt in range(API_RETRY[\"max_retries\"]):\n        try:\n            return api_func()\n        except Exception as e:\n            error_str = str(e).lower()\n            is_retryable = any(err in error_str for err in API_RETRY[\"retryable_errors\"])\n            \n            if attempt == API_RETRY[\"max_retries\"] - 1 or not is_retryable:\n                raise\n            \n            delay = API_RETRY[\"base_delay\"] * (2 ** attempt)\n            print(f\"  ⚠ {description} failed (attempt {attempt + 1}/{API_RETRY['max_retries']}), retrying in {delay}s...\")\n            time.sleep(delay)\n\n\nprint(\"✓ Configuration loaded\")\nprint(f\"  Model: {MODEL}\")\nprint(f\"  Max tokens: {MAX_TOKENS}\")\nprint(f\"  Max pages per chunk: {PDF_LIMITS['chunk_size_pages']}\")\nprint(f\"  Input: {INPUT_PDF}\")\nprint(f\"  Output: {OUTPUT_MD}\")"
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 2b: PDF Validation & Chunking Functions",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def get_pdf_info(pdf_path: str) -> dict:\n    \"\"\"\n    Get PDF metadata including page count and file size.\n    \n    Returns:\n        dict: {\n            \"exists\": bool,\n            \"file_size_mb\": float,\n            \"page_count\": int,\n            \"needs_chunking\": bool,\n            \"estimated_chunks\": int,\n            \"error\": str|None\n        }\n    \"\"\"\n    path = Path(pdf_path)\n    \n    if not path.exists():\n        return {\n            \"exists\": False,\n            \"file_size_mb\": 0,\n            \"page_count\": 0,\n            \"needs_chunking\": False,\n            \"estimated_chunks\": 0,\n            \"error\": \"File not found\"\n        }\n    \n    try:\n        file_size_mb = path.stat().st_size / (1024 * 1024)\n        \n        reader = PdfReader(pdf_path)\n        page_count = len(reader.pages)\n        \n        # Determine if chunking is needed\n        exceeds_pages = page_count > PDF_LIMITS[\"max_pages_per_request\"]\n        exceeds_size = file_size_mb > PDF_LIMITS[\"max_file_size_mb\"]\n        needs_chunking = exceeds_pages or exceeds_size\n        \n        # Estimate chunks needed\n        if needs_chunking:\n            chunks_by_pages = (page_count + PDF_LIMITS[\"chunk_size_pages\"] - 1) // PDF_LIMITS[\"chunk_size_pages\"]\n            chunks_by_size = (file_size_mb / PDF_LIMITS[\"chunk_size_mb\"]) if exceeds_size else 1\n            estimated_chunks = max(chunks_by_pages, int(chunks_by_size) + 1)\n        else:\n            estimated_chunks = 1\n        \n        return {\n            \"exists\": True,\n            \"file_size_mb\": file_size_mb,\n            \"page_count\": page_count,\n            \"needs_chunking\": needs_chunking,\n            \"estimated_chunks\": estimated_chunks,\n            \"exceeds_pages\": exceeds_pages,\n            \"exceeds_size\": exceeds_size,\n            \"error\": None\n        }\n        \n    except Exception as e:\n        return {\n            \"exists\": True,\n            \"file_size_mb\": path.stat().st_size / (1024 * 1024),\n            \"page_count\": 0,\n            \"needs_chunking\": False,\n            \"estimated_chunks\": 0,\n            \"error\": str(e)\n        }\n\n\ndef chunk_pdf(pdf_path: str, output_dir: str = None) -> dict:\n    \"\"\"\n    Split a large PDF into smaller chunks suitable for Claude Vision API.\n    \n    Args:\n        pdf_path: Path to the PDF file\n        output_dir: Directory for chunk files (uses temp dir if None)\n    \n    Returns:\n        dict: {\n            \"success\": bool,\n            \"chunks\": list of {\"path\": str, \"start_page\": int, \"end_page\": int, \"size_mb\": float},\n            \"total_pages\": int,\n            \"error\": str|None\n        }\n    \"\"\"\n    try:\n        reader = PdfReader(pdf_path)\n        total_pages = len(reader.pages)\n        \n        # Use temp directory if not specified\n        if output_dir is None:\n            output_dir = tempfile.mkdtemp(prefix=\"pdf_chunks_\")\n        else:\n            Path(output_dir).mkdir(parents=True, exist_ok=True)\n        \n        chunks = []\n        chunk_num = 0\n        current_page = 0\n        \n        while current_page < total_pages:\n            chunk_num += 1\n            start_page = current_page\n            end_page = min(current_page + PDF_LIMITS[\"chunk_size_pages\"], total_pages)\n            \n            # Create chunk PDF\n            writer = PdfWriter()\n            for page_idx in range(start_page, end_page):\n                writer.add_page(reader.pages[page_idx])\n            \n            # Write chunk to file\n            chunk_filename = f\"chunk_{chunk_num:03d}_pages_{start_page+1}-{end_page}.pdf\"\n            chunk_path = Path(output_dir) / chunk_filename\n            \n            with open(chunk_path, \"wb\") as f:\n                writer.write(f)\n            \n            chunk_size_mb = chunk_path.stat().st_size / (1024 * 1024)\n            \n            # Check if chunk exceeds size limit - if so, we need to split further\n            if chunk_size_mb > PDF_LIMITS[\"max_file_size_mb\"] and (end_page - start_page) > 1:\n                # Remove the oversized chunk and recurse with smaller page count\n                chunk_path.unlink()\n                \n                # Binary search for appropriate chunk size\n                pages_to_try = (end_page - start_page) // 2\n                while pages_to_try > 0:\n                    writer = PdfWriter()\n                    test_end = start_page + pages_to_try\n                    for page_idx in range(start_page, test_end):\n                        writer.add_page(reader.pages[page_idx])\n                    \n                    with open(chunk_path, \"wb\") as f:\n                        writer.write(f)\n                    \n                    chunk_size_mb = chunk_path.stat().st_size / (1024 * 1024)\n                    \n                    if chunk_size_mb <= PDF_LIMITS[\"max_file_size_mb\"]:\n                        end_page = test_end\n                        break\n                    else:\n                        chunk_path.unlink()\n                        pages_to_try = pages_to_try // 2\n                \n                if pages_to_try == 0:\n                    # Single page exceeds limit - this is a problem\n                    return {\n                        \"success\": False,\n                        \"chunks\": [],\n                        \"total_pages\": total_pages,\n                        \"error\": f\"Page {start_page + 1} exceeds {PDF_LIMITS['max_file_size_mb']}MB limit\"\n                    }\n            \n            chunks.append({\n                \"path\": str(chunk_path),\n                \"start_page\": start_page + 1,  # 1-indexed for display\n                \"end_page\": end_page,\n                \"size_mb\": chunk_size_mb\n            })\n            \n            current_page = end_page\n        \n        return {\n            \"success\": True,\n            \"chunks\": chunks,\n            \"total_pages\": total_pages,\n            \"output_dir\": output_dir,\n            \"error\": None\n        }\n        \n    except Exception as e:\n        return {\n            \"success\": False,\n            \"chunks\": [],\n            \"total_pages\": 0,\n            \"error\": str(e)\n        }\n\n\ndef process_chunks_with_vision(chunks: list, verbose: bool = True) -> dict:\n    \"\"\"\n    Process PDF chunks through Claude Vision and combine results.\n    \n    Args:\n        chunks: List of chunk info dicts from chunk_pdf()\n        verbose: Print progress\n    \n    Returns:\n        dict: {\n            \"success\": bool,\n            \"markdown\": str,\n            \"tokens_used\": int,\n            \"chunk_results\": list,\n            \"error\": str|None\n        }\n    \"\"\"\n    all_markdown = []\n    total_tokens = 0\n    chunk_results = []\n    \n    for i, chunk in enumerate(chunks, 1):\n        if verbose:\n            print(f\"    Processing chunk {i}/{len(chunks)} (pages {chunk['start_page']}-{chunk['end_page']})...\", end=\" \")\n        \n        result = claude_vision_extract(chunk[\"path\"])\n        \n        chunk_results.append({\n            \"chunk\": i,\n            \"pages\": f\"{chunk['start_page']}-{chunk['end_page']}\",\n            \"success\": result[\"success\"],\n            \"tokens\": result.get(\"tokens_used\", 0),\n            \"error\": result.get(\"error\")\n        })\n        \n        if result[\"success\"]:\n            # Add page range header for context\n            header = f\"\\n\\n<!-- Pages {chunk['start_page']}-{chunk['end_page']} -->\\n\\n\"\n            all_markdown.append(header + result[\"markdown\"])\n            total_tokens += result.get(\"tokens_used\", 0)\n            \n            if verbose:\n                print(f\"✓ ({result.get('tokens_used', 0):,} tokens)\")\n        else:\n            if verbose:\n                print(f\"✗ {result.get('error', 'Unknown error')}\")\n            \n            # Continue processing other chunks even if one fails\n            all_markdown.append(f\"\\n\\n<!-- Pages {chunk['start_page']}-{chunk['end_page']}: EXTRACTION FAILED -->\\n\\n\")\n    \n    # Combine all markdown\n    combined_markdown = \"\\n\".join(all_markdown)\n    \n    # Check if any chunks succeeded\n    successful_chunks = sum(1 for r in chunk_results if r[\"success\"])\n    \n    return {\n        \"success\": successful_chunks > 0,\n        \"markdown\": combined_markdown,\n        \"tokens_used\": total_tokens,\n        \"chunks_processed\": len(chunks),\n        \"chunks_successful\": successful_chunks,\n        \"chunk_results\": chunk_results,\n        \"error\": None if successful_chunks > 0 else \"All chunks failed\"\n    }\n\n\ndef cleanup_chunks(output_dir: str) -> None:\n    \"\"\"Remove temporary chunk files.\"\"\"\n    import shutil\n    try:\n        shutil.rmtree(output_dir)\n    except Exception:\n        pass  # Best effort cleanup\n\n\nprint(\"✓ PDF validation and chunking functions loaded\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell3_header"
   },
   "source": [
    "## Cell 3: Quality Assessment Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "quality_functions",
    "outputId": "498b54c5-1d81-4ace-8b2f-6d63302088ad"
   },
   "outputs": [],
   "source": "def heuristic_check(markdown: str, page_count: int = 1) -> dict:\n    \"\"\"\n    Perform fast, free heuristic checks on extracted markdown.\n\n    Returns:\n        dict: {\"passed\": bool, \"issues\": list, \"metrics\": dict}\n    \"\"\"\n    issues = []\n\n    # Metric calculations\n    total_chars = len(markdown)\n    \n    # Efficient whitespace calculation (avoids character-by-character iteration)\n    non_whitespace_chars = len(''.join(markdown.split()))\n    whitespace_chars = total_chars - non_whitespace_chars\n    whitespace_ratio = whitespace_chars / total_chars if total_chars > 0 else 1\n\n    words = markdown.split()\n    word_count = len(words)\n    words_per_page = word_count / max(page_count, 1)\n\n    header_count = len(re.findall(r'^#{1,6}\\s', markdown, re.MULTILINE))\n\n    # Check for table integrity (pipes with inconsistent columns)\n    table_lines = [l for l in markdown.split('\\n') if '|' in l]\n    table_issues = False\n    if table_lines:\n        col_counts = [l.count('|') for l in table_lines]\n        if col_counts and max(col_counts) - min(col_counts) > 2:\n            table_issues = True\n\n    # Check for repeated character artifacts (OCR noise)\n    artifact_patterns = [\n        r'\\.{5,}',      # Multiple dots\n        r'\\|{3,}',      # Multiple pipes\n        r'_{5,}',       # Multiple underscores\n        r'\\s{10,}',     # Excessive whitespace\n    ]\n    artifact_count = sum(len(re.findall(p, markdown)) for p in artifact_patterns)\n\n    # Evaluate against thresholds\n    if words_per_page < QUALITY_THRESHOLDS[\"min_words_per_page\"]:\n        issues.append(f\"Low word count: {words_per_page:.0f} words/page (min: {QUALITY_THRESHOLDS['min_words_per_page']})\")\n\n    if whitespace_ratio > QUALITY_THRESHOLDS[\"max_whitespace_ratio\"]:\n        issues.append(f\"High whitespace ratio: {whitespace_ratio:.1%} (max: {QUALITY_THRESHOLDS['max_whitespace_ratio']:.0%})\")\n\n    if word_count > 500 and header_count < QUALITY_THRESHOLDS[\"min_headers_for_long_doc\"]:\n        issues.append(f\"No headers found in long document ({word_count} words)\")\n\n    if table_issues:\n        issues.append(\"Inconsistent table structure detected\")\n\n    if artifact_count > 10:\n        issues.append(f\"Excessive extraction artifacts detected ({artifact_count} patterns)\")\n\n    metrics = {\n        \"word_count\": word_count,\n        \"words_per_page\": words_per_page,\n        \"whitespace_ratio\": whitespace_ratio,\n        \"header_count\": header_count,\n        \"artifact_count\": artifact_count,\n        \"table_line_count\": len(table_lines),\n    }\n\n    return {\n        \"passed\": len(issues) == 0,\n        \"issues\": issues,\n        \"metrics\": metrics\n    }\n\n\ndef claude_quality_check(markdown: str) -> dict:\n    \"\"\"\n    Use Claude to assess the quality of extracted markdown.\n\n    Returns:\n        dict: {\"score\": int, \"issues\": list, \"tokens_used\": int}\n    \"\"\"\n    # Truncate if too long to save tokens (using config value)\n    sample_limit = QUALITY_THRESHOLDS[\"quality_sample_chars\"]\n    sample = markdown[:sample_limit] if len(markdown) > sample_limit else markdown\n\n    prompt = f\"\"\"Evaluate this markdown extraction from a PDF. Rate the quality from 1-10 based on:\n- Completeness (does it appear to capture all content?)\n- Formatting (are headers, lists, tables properly structured?)\n- Readability (is it clean and well-organized?)\n- Artifacts (are there extraction errors, garbled text, or noise?)\n\nRespond with ONLY a JSON object, no other text:\n{{\"score\": N, \"issues\": [\"issue1\", \"issue2\"]}}\n\nIf there are no issues, return an empty array for issues.\n\n---\nMARKDOWN TO EVALUATE:\n{sample}\n---\"\"\"\n\n    try:\n        response = api_call_with_retry(\n            lambda: client.messages.create(\n                model=MODEL,\n                max_tokens=500,\n                messages=[{\"role\": \"user\", \"content\": prompt}]\n            ),\n            description=\"Quality check\"\n        )\n\n        result_text = response.content[0].text.strip()\n\n        # Clean up response if needed\n        if result_text.startswith('```'):\n            result_text = re.sub(r'^```json?\\n?', '', result_text)\n            result_text = re.sub(r'\\n?```$', '', result_text)\n\n        result = json.loads(result_text)\n        return {\n            \"score\": result.get(\"score\", 0),\n            \"issues\": result.get(\"issues\", []),\n            \"tokens_used\": response.usage.input_tokens + response.usage.output_tokens\n        }\n\n    except json.JSONDecodeError as e:\n        print(f\"  ⚠ Failed to parse quality response: {e}\")\n        return {\"score\": 0, \"issues\": [\"Failed to parse quality assessment\"], \"tokens_used\": 0}\n    except Exception as e:\n        print(f\"  ⚠ Quality check error: {e}\")\n        return {\"score\": 0, \"issues\": [str(e)], \"tokens_used\": 0}\n\n\nprint(\"✓ Quality assessment functions loaded\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell4_header"
   },
   "source": [
    "## Cell 4: Conversion Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "conversion_functions",
    "outputId": "eab0943a-2c3f-48cc-8338-80edc550eb68"
   },
   "outputs": [],
   "source": "def markitdown_extract(pdf_path: str) -> dict:\n    \"\"\"\n    Extract markdown from PDF using MarkItDown.\n\n    Returns:\n        dict: {\"success\": bool, \"markdown\": str, \"error\": str|None}\n    \"\"\"\n    try:\n        result = markitdown.convert(pdf_path)\n        return {\n            \"success\": True,\n            \"markdown\": result.text_content,\n            \"error\": None\n        }\n    except Exception as e:\n        return {\n            \"success\": False,\n            \"markdown\": \"\",\n            \"error\": str(e)\n        }\n\n\ndef claude_vision_extract(pdf_path: str) -> dict:\n    \"\"\"\n    Extract markdown from PDF using Claude's native PDF/vision capability.\n\n    Returns:\n        dict: {\"success\": bool, \"markdown\": str, \"tokens_used\": int, \"error\": str|None}\n    \"\"\"\n    try:\n        # Read and encode PDF\n        with open(pdf_path, \"rb\") as f:\n            pdf_data = base64.standard_b64encode(f.read()).decode(\"utf-8\")\n\n        prompt = \"\"\"Convert this PDF to clean, well-structured Markdown. Follow these guidelines:\n\n1. **Headers**: Use appropriate header levels (# ## ###) to reflect document hierarchy\n2. **Tables**: Convert all tables to proper markdown table format\n3. **Lists**: Use bullet points or numbered lists where appropriate\n4. **Content**: Preserve ALL text content accurately - do not summarize or omit\n5. **Charts/Graphics**: Add descriptive placeholders like [Chart: Revenue by Quarter] or [Image: Company Logo]\n6. **Formatting**: Remove extraction artifacts, fix spacing issues, ensure clean output\n\nOutput ONLY the markdown content, no explanations or preamble.\"\"\"\n\n        response = api_call_with_retry(\n            lambda: client.messages.create(\n                model=MODEL,\n                max_tokens=MAX_TOKENS,\n                messages=[\n                    {\n                        \"role\": \"user\",\n                        \"content\": [\n                            {\n                                \"type\": \"document\",\n                                \"source\": {\n                                    \"type\": \"base64\",\n                                    \"media_type\": \"application/pdf\",\n                                    \"data\": pdf_data\n                                }\n                            },\n                            {\n                                \"type\": \"text\",\n                                \"text\": prompt\n                            }\n                        ]\n                    }\n                ]\n            ),\n            description=\"Vision extraction\"\n        )\n\n        return {\n            \"success\": True,\n            \"markdown\": response.content[0].text,\n            \"tokens_used\": response.usage.input_tokens + response.usage.output_tokens,\n            \"error\": None\n        }\n\n    except Exception as e:\n        return {\n            \"success\": False,\n            \"markdown\": \"\",\n            \"tokens_used\": 0,\n            \"error\": str(e)\n        }\n\n\ndef claude_cleanup(markdown: str) -> dict:\n    \"\"\"\n    Clean up and improve MarkItDown output using Claude.\n\n    Returns:\n        dict: {\"success\": bool, \"markdown\": str, \"tokens_used\": int, \"error\": str|None}\n    \"\"\"\n    try:\n        prompt = f\"\"\"Clean up and improve this markdown extracted from a PDF. Your task:\n\n1. Fix any formatting inconsistencies (headers, lists, tables)\n2. Remove extraction artifacts and noise\n3. Ensure proper markdown table formatting\n4. Fix spacing and line break issues\n5. Preserve ALL original content - do not summarize or remove information\n6. Use consistent header hierarchy\n\nOutput ONLY the cleaned markdown, no explanations.\n\n---\nMARKDOWN TO CLEAN:\n{markdown}\n---\"\"\"\n\n        response = api_call_with_retry(\n            lambda: client.messages.create(\n                model=MODEL,\n                max_tokens=MAX_TOKENS,\n                messages=[{\"role\": \"user\", \"content\": prompt}]\n            ),\n            description=\"Cleanup\"\n        )\n\n        return {\n            \"success\": True,\n            \"markdown\": response.content[0].text,\n            \"tokens_used\": response.usage.input_tokens + response.usage.output_tokens,\n            \"error\": None\n        }\n\n    except Exception as e:\n        return {\n            \"success\": False,\n            \"markdown\": markdown,  # Return original on failure\n            \"tokens_used\": 0,\n            \"error\": str(e)\n        }\n\n\nprint(\"✓ Conversion functions loaded\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell5_header"
   },
   "source": [
    "## Cell 5: Main Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "main_pipeline",
    "outputId": "6709a474-e39a-41b0-9bea-6764320803b0"
   },
   "outputs": [],
   "source": "def claude_vision_with_chunking(pdf_path: str, pdf_info: dict, verbose: bool = True) -> dict:\n    \"\"\"\n    Process PDF through Claude Vision, automatically chunking if needed.\n    \n    Args:\n        pdf_path: Path to PDF file\n        pdf_info: PDF info dict from get_pdf_info()\n        verbose: Print progress\n    \n    Returns:\n        dict with success, markdown, tokens_used, etc.\n    \"\"\"\n    if not pdf_info[\"needs_chunking\"]:\n        # Small PDF - process directly\n        return claude_vision_extract(pdf_path)\n    \n    # Large PDF - need to chunk\n    if verbose:\n        print(f\"  PDF requires chunking ({pdf_info['page_count']} pages, {pdf_info['file_size_mb']:.1f}MB)\")\n        print(f\"  Splitting into ~{pdf_info['estimated_chunks']} chunks...\")\n    \n    chunk_result = chunk_pdf(pdf_path)\n    \n    if not chunk_result[\"success\"]:\n        return {\n            \"success\": False,\n            \"markdown\": \"\",\n            \"tokens_used\": 0,\n            \"error\": f\"Chunking failed: {chunk_result['error']}\"\n        }\n    \n    if verbose:\n        print(f\"  Created {len(chunk_result['chunks'])} chunks:\")\n        for chunk in chunk_result[\"chunks\"]:\n            print(f\"    - Pages {chunk['start_page']}-{chunk['end_page']}: {chunk['size_mb']:.1f}MB\")\n        print()\n    \n    # Process all chunks\n    vision_result = process_chunks_with_vision(chunk_result[\"chunks\"], verbose=verbose)\n    \n    # Cleanup temporary files\n    cleanup_chunks(chunk_result[\"output_dir\"])\n    \n    return vision_result\n\n\ndef process_pdf(pdf_path: str, verbose: bool = True) -> dict:\n    \"\"\"\n    Main processing pipeline with automatic quality detection, fallback, and chunking.\n\n    Args:\n        pdf_path: Path to the PDF file\n        verbose: Print progress and diagnostics\n\n    Returns:\n        dict: {\n            \"success\": bool,\n            \"markdown\": str,\n            \"path_taken\": str,\n            \"total_tokens\": int,\n            \"diagnostics\": dict\n        }\n    \"\"\"\n    diagnostics = {\n        \"pdf_info\": None,\n        \"markitdown_extraction\": None,\n        \"heuristic_check\": None,\n        \"claude_quality_check\": None,\n        \"final_conversion\": None\n    }\n    total_tokens = 0\n\n    if verbose:\n        print(f\"\\n{'='*60}\")\n        print(f\"Processing: {pdf_path}\")\n        print(f\"{'='*60}\\n\")\n\n    # =========================================================================\n    # Step 0: PDF Validation & Info\n    # =========================================================================\n    if verbose:\n        print(\"[0/4] PDF Validation...\")\n\n    pdf_info = get_pdf_info(pdf_path)\n    diagnostics[\"pdf_info\"] = pdf_info\n\n    if not pdf_info[\"exists\"]:\n        if verbose:\n            print(f\"  ✗ {pdf_info['error']}\\n\")\n        return {\n            \"success\": False,\n            \"markdown\": \"\",\n            \"path_taken\": \"Failed (file not found)\",\n            \"total_tokens\": 0,\n            \"diagnostics\": diagnostics\n        }\n\n    if pdf_info[\"error\"]:\n        if verbose:\n            print(f\"  ⚠ Warning: {pdf_info['error']}\")\n\n    if verbose:\n        print(f\"  File size: {pdf_info['file_size_mb']:.2f} MB\")\n        print(f\"  Page count: {pdf_info['page_count']}\")\n        if pdf_info[\"needs_chunking\"]:\n            reasons = []\n            if pdf_info.get(\"exceeds_pages\"):\n                reasons.append(f\">{PDF_LIMITS['max_pages_per_request']} pages\")\n            if pdf_info.get(\"exceeds_size\"):\n                reasons.append(f\">{PDF_LIMITS['max_file_size_mb']}MB\")\n            print(f\"  Chunking required: Yes ({', '.join(reasons)})\")\n            print(f\"  Estimated chunks: {pdf_info['estimated_chunks']}\")\n        else:\n            print(f\"  Chunking required: No\")\n        print()\n\n    # =========================================================================\n    # Step 1: MarkItDown Extraction\n    # =========================================================================\n    if verbose:\n        print(\"[1/4] MarkItDown Extraction...\")\n\n    mit_result = markitdown_extract(pdf_path)\n    diagnostics[\"markitdown_extraction\"] = {\n        \"success\": mit_result[\"success\"],\n        \"char_count\": len(mit_result[\"markdown\"]) if mit_result[\"success\"] else 0,\n        \"error\": mit_result[\"error\"]\n    }\n\n    if not mit_result[\"success\"]:\n        if verbose:\n            print(f\"  ✗ MarkItDown failed: {mit_result['error']}\")\n            print(\"  → Falling back to Claude Vision...\\n\")\n\n        # Direct fallback to Claude Vision (with chunking if needed)\n        vision_result = claude_vision_with_chunking(pdf_path, pdf_info, verbose)\n        total_tokens += vision_result.get(\"tokens_used\", 0)\n        \n        diagnostics[\"final_conversion\"] = {\n            \"method\": \"claude_vision_fallback\",\n            \"reason\": \"markitdown_failed\",\n            \"tokens\": vision_result.get(\"tokens_used\", 0),\n            \"chunked\": pdf_info[\"needs_chunking\"],\n            \"chunks_processed\": vision_result.get(\"chunks_processed\", 1)\n        }\n\n        return {\n            \"success\": vision_result[\"success\"],\n            \"markdown\": vision_result[\"markdown\"],\n            \"path_taken\": f\"Claude Vision (MarkItDown failed){' [chunked]' if pdf_info['needs_chunking'] else ''}\",\n            \"total_tokens\": total_tokens,\n            \"diagnostics\": diagnostics\n        }\n\n    if verbose:\n        print(f\"  ✓ Extracted {len(mit_result['markdown']):,} characters\\n\")\n\n    # =========================================================================\n    # Step 2: Heuristic Quality Check\n    # =========================================================================\n    if verbose:\n        print(\"[2/4] Heuristic Quality Check...\")\n\n    # Use actual page count from PDF info\n    page_count = pdf_info[\"page_count\"] if pdf_info[\"page_count\"] > 0 else max(1, len(mit_result[\"markdown\"]) // 3000)\n    heuristic_result = heuristic_check(mit_result[\"markdown\"], page_count)\n    diagnostics[\"heuristic_check\"] = heuristic_result\n\n    if verbose:\n        print(f\"  Metrics:\")\n        for key, value in heuristic_result[\"metrics\"].items():\n            if isinstance(value, float):\n                print(f\"    - {key}: {value:.2f}\")\n            else:\n                print(f\"    - {key}: {value}\")\n\n    if not heuristic_result[\"passed\"]:\n        if verbose:\n            print(f\"  ✗ Heuristic check failed:\")\n            for issue in heuristic_result[\"issues\"]:\n                print(f\"    - {issue}\")\n            print(\"  → Falling back to Claude Vision...\\n\")\n\n        vision_result = claude_vision_with_chunking(pdf_path, pdf_info, verbose)\n        total_tokens += vision_result.get(\"tokens_used\", 0)\n        \n        diagnostics[\"final_conversion\"] = {\n            \"method\": \"claude_vision_fallback\",\n            \"reason\": \"heuristic_check_failed\",\n            \"tokens\": vision_result.get(\"tokens_used\", 0),\n            \"chunked\": pdf_info[\"needs_chunking\"],\n            \"chunks_processed\": vision_result.get(\"chunks_processed\", 1)\n        }\n\n        if verbose and not pdf_info[\"needs_chunking\"]:\n            print(f\"  {'✓' if vision_result['success'] else '✗'} Claude Vision: {vision_result.get('tokens_used', 0):,} tokens\\n\")\n\n        return {\n            \"success\": vision_result[\"success\"],\n            \"markdown\": vision_result[\"markdown\"],\n            \"path_taken\": f\"Claude Vision (Heuristic failed){' [chunked]' if pdf_info['needs_chunking'] else ''}\",\n            \"total_tokens\": total_tokens,\n            \"diagnostics\": diagnostics\n        }\n\n    if verbose:\n        print(f\"  ✓ Heuristic check passed\\n\")\n\n    # =========================================================================\n    # Step 3: Claude Quality Assessment\n    # =========================================================================\n    if verbose:\n        print(\"[3/4] Claude Quality Assessment...\")\n\n    quality_result = claude_quality_check(mit_result[\"markdown\"])\n    total_tokens += quality_result.get(\"tokens_used\", 0)\n    diagnostics[\"claude_quality_check\"] = quality_result\n\n    if verbose:\n        print(f\"  Score: {quality_result['score']}/10\")\n        if quality_result[\"issues\"]:\n            print(f\"  Issues:\")\n            for issue in quality_result[\"issues\"]:\n                print(f\"    - {issue}\")\n        print(f\"  Tokens used: {quality_result.get('tokens_used', 0):,}\\n\")\n\n    if quality_result[\"score\"] < QUALITY_THRESHOLDS[\"min_quality_score\"]:\n        if verbose:\n            print(f\"  ✗ Quality score below threshold ({QUALITY_THRESHOLDS['min_quality_score']})\")\n            print(\"  → Falling back to Claude Vision...\\n\")\n\n        vision_result = claude_vision_with_chunking(pdf_path, pdf_info, verbose)\n        total_tokens += vision_result.get(\"tokens_used\", 0)\n        \n        diagnostics[\"final_conversion\"] = {\n            \"method\": \"claude_vision_fallback\",\n            \"reason\": f\"quality_score_{quality_result['score']}_below_{QUALITY_THRESHOLDS['min_quality_score']}\",\n            \"tokens\": vision_result.get(\"tokens_used\", 0),\n            \"chunked\": pdf_info[\"needs_chunking\"],\n            \"chunks_processed\": vision_result.get(\"chunks_processed\", 1)\n        }\n\n        if verbose and not pdf_info[\"needs_chunking\"]:\n            print(f\"  {'✓' if vision_result['success'] else '✗'} Claude Vision: {vision_result.get('tokens_used', 0):,} tokens\\n\")\n\n        return {\n            \"success\": vision_result[\"success\"],\n            \"markdown\": vision_result[\"markdown\"],\n            \"path_taken\": f\"Claude Vision (Quality score: {quality_result['score']}/10){' [chunked]' if pdf_info['needs_chunking'] else ''}\",\n            \"total_tokens\": total_tokens,\n            \"diagnostics\": diagnostics\n        }\n\n    # =========================================================================\n    # Step 4: Skip Cleanup or Apply Cleanup\n    # =========================================================================\n    if quality_result[\"score\"] >= QUALITY_THRESHOLDS[\"skip_cleanup_score\"]:\n        if verbose:\n            print(f\"  ✓ Quality excellent ({quality_result['score']}/10), skipping cleanup\\n\")\n        \n        diagnostics[\"final_conversion\"] = {\n            \"method\": \"markitdown_no_cleanup\",\n            \"reason\": f\"quality_score_{quality_result['score']}_excellent\",\n            \"tokens\": 0\n        }\n\n        return {\n            \"success\": True,\n            \"markdown\": mit_result[\"markdown\"],\n            \"path_taken\": \"MarkItDown (no cleanup needed)\",\n            \"total_tokens\": total_tokens,\n            \"diagnostics\": diagnostics\n        }\n\n    if verbose:\n        print(f\"  ✓ Quality check passed\\n\")\n        print(\"[4/4] Claude Cleanup...\")\n\n    cleanup_result = claude_cleanup(mit_result[\"markdown\"])\n    total_tokens += cleanup_result.get(\"tokens_used\", 0)\n    diagnostics[\"final_conversion\"] = {\n        \"method\": \"markitdown_plus_cleanup\",\n        \"reason\": \"quality_check_passed\",\n        \"tokens\": cleanup_result.get(\"tokens_used\", 0)\n    }\n\n    if verbose:\n        print(f\"  {'✓' if cleanup_result['success'] else '✗'} Cleanup complete: {cleanup_result.get('tokens_used', 0):,} tokens\\n\")\n\n    return {\n        \"success\": cleanup_result[\"success\"],\n        \"markdown\": cleanup_result[\"markdown\"],\n        \"path_taken\": \"MarkItDown + Claude Cleanup\",\n        \"total_tokens\": total_tokens,\n        \"diagnostics\": diagnostics\n    }\n\n\nprint(\"✓ Main pipeline loaded (with large PDF support)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell6_header"
   },
   "source": [
    "## Cell 6: Execute Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "execute",
    "outputId": "9cdb61aa-1f4f-4950-bc8d-4f2f19dc5434"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "============================================================\n",
      "Processing: /content/pdfs/Ryder-Investor-Overview-June-2024.pdf\n",
      "============================================================\n",
      "\n",
      "[1/4] MarkItDown Extraction...\n",
      "  ✓ Extracted 25,632 characters\n",
      "\n",
      "[2/4] Heuristic Quality Check...\n",
      "  Metrics:\n",
      "    - word_count: 3688\n",
      "    - words_per_page: 461.00\n",
      "    - whitespace_ratio: 0.17\n",
      "    - header_count: 0\n",
      "    - artifact_count: 4\n",
      "    - table_line_count: 0\n",
      "  ✗ Heuristic check failed:\n",
      "    - No headers found in long document (3688 words)\n",
      "  → Falling back to Claude Vision...\n",
      "\n",
      "  ✓ Claude Vision: 49,046 tokens\n",
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "Status: ✓ Success\n",
      "Path taken: Claude Vision (Heuristic failed)\n",
      "Total tokens used: 49,046\n",
      "Output length: 27,500 characters\n",
      "\n",
      "✓ Saved to: /content/output.md\n",
      "\n",
      "============================================================\n",
      "PREVIEW (first 2000 characters)\n",
      "============================================================\n",
      "\n",
      "# Investor Overview\n",
      "## June 2024\n",
      "\n",
      "![Ryder Logo](ryder-logo.png)\n",
      "\n",
      "![Fleet and Operations Image](ryder-operations.png)\n",
      "\n",
      "---\n",
      "\n",
      "## Safe Harbor and Non-GAAP Financial Measures\n",
      "\n",
      "### Note Regarding Forward-Looking Statements\n",
      "\n",
      "Certain statements and information included in this news release are \"forward-looking statements\" under the Federal Private Securities Litigation Reform Act of 1995, including:\n",
      "\n",
      "- Our forecast and outlook\n",
      "- Our expectations regarding market trends and economic environment, such as:\n",
      "  - Rental demand\n",
      "  - Economic growth\n",
      "  - Challenging freight environment\n",
      "  - Weakening used vehicle sales and rental\n",
      "  - Declining volumes in our omnichannel retail vertical\n",
      "- Our expectations regarding the freight cycle, including timing and impact on our businesses\n",
      "- Our expectations regarding total and operating revenue, earnings per share, comparable earnings per share, adjusted ROE, earnings before income tax, net cash from operating activities from continuing operations, debt-to-equity, capital expenditures, operating cash flow, free cash flow\n",
      "- Our ability to execute our balanced growth strategy\n",
      "- The impact of inflationary pressures and inflationary cost recovery\n",
      "- Our expectations regarding commercial rental demand and utilization and used vehicle sales volume and pricing\n",
      "- Our expectations with respect to ChoiceLease growth\n",
      "- Our expectations with respect to the timing of OEM deliveries and vehicle production\n",
      "- Our expectations with respect to our actions to increase returns and create long-term value\n",
      "- Our ability to outperform prior cycles\n",
      "- Our expectations regarding long-term profitable growth and secular growth trends\n",
      "- Our ability to profitably grow SCS/DTS\n",
      "- Benefits from FMS lease pricing and maintenance cost savings initiatives\n",
      "- Our expectations regarding used vehicle inventory and fleet size\n",
      "- Our ability to redeploy rental vehicles and leverage our expanded retail used vehicle network\n",
      "- Our ability to execute our enhanced asset management playbook\n",
      "- Ou\n",
      "\n",
      "... [25,500 more characters]\n"
     ]
    }
   ],
   "source": [
    "# Check if input file exists\n",
    "if not Path(INPUT_PDF).exists():\n",
    "    print(f\"❌ Error: Input file not found: {INPUT_PDF}\")\n",
    "    print(\"\\nPlease upload your PDF to /content/ or update INPUT_PDF path in Cell 2.\")\n",
    "else:\n",
    "    # Process the PDF\n",
    "    result = process_pdf(INPUT_PDF, verbose=True)\n",
    "\n",
    "    # Summary\n",
    "    print(f\"{'='*60}\")\n",
    "    print(\"SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Status: {'✓ Success' if result['success'] else '✗ Failed'}\")\n",
    "    print(f\"Path taken: {result['path_taken']}\")\n",
    "    print(f\"Total tokens used: {result['total_tokens']:,}\")\n",
    "    print(f\"Output length: {len(result['markdown']):,} characters\")\n",
    "\n",
    "    if result[\"success\"]:\n",
    "        # Save output\n",
    "        with open(OUTPUT_MD, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(result[\"markdown\"])\n",
    "        print(f\"\\n✓ Saved to: {OUTPUT_MD}\")\n",
    "\n",
    "        # Preview\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"PREVIEW (first 2000 characters)\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        print(result[\"markdown\"][:2000])\n",
    "        if len(result[\"markdown\"]) > 2000:\n",
    "            print(f\"\\n... [{len(result['markdown']) - 2000:,} more characters]\")\n",
    "    else:\n",
    "        print(f\"\\n❌ Conversion failed. Check diagnostics above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell7_header"
   },
   "source": [
    "## Cell 7: Batch Processing (Optional)\n",
    "\n",
    "Use this cell to process multiple PDFs in a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "batch_processing"
   },
   "outputs": [],
   "source": "def batch_process(input_dir: str, output_dir: str, verbose: bool = False) -> list:\n    \"\"\"\n    Process all PDFs in a directory with robust error handling.\n\n    Args:\n        input_dir: Directory containing PDF files\n        output_dir: Directory to save markdown files\n        verbose: Print detailed progress for each file\n\n    Returns:\n        list: Results for each processed file\n    \"\"\"\n    input_path = Path(input_dir)\n    output_path = Path(output_dir)\n    output_path.mkdir(parents=True, exist_ok=True)\n\n    pdf_files = list(input_path.glob(\"*.pdf\")) + list(input_path.glob(\"*.PDF\"))\n\n    if not pdf_files:\n        print(f\"No PDF files found in {input_dir}\")\n        return []\n\n    print(f\"Found {len(pdf_files)} PDF files to process\\n\")\n\n    results = []\n    total_tokens = 0\n    failed_files = []\n\n    for i, pdf_file in enumerate(pdf_files, 1):\n        print(f\"[{i}/{len(pdf_files)}] {pdf_file.name}...\", end=\" \")\n\n        try:\n            result = process_pdf(str(pdf_file), verbose=verbose)\n            result[\"filename\"] = pdf_file.name\n            results.append(result)\n            total_tokens += result[\"total_tokens\"]\n\n            if result[\"success\"]:\n                output_file = output_path / f\"{pdf_file.stem}.md\"\n                with open(output_file, \"w\", encoding=\"utf-8\") as f:\n                    f.write(result[\"markdown\"])\n                print(f\"✓ ({result['total_tokens']:,} tokens)\")\n            else:\n                print(f\"✗ Failed: {result.get('diagnostics', {}).get('final_conversion', {}).get('error', 'Unknown error')}\")\n                failed_files.append(pdf_file.name)\n\n        except Exception as e:\n            error_msg = str(e)\n            print(f\"✗ Error: {error_msg}\")\n            failed_files.append(pdf_file.name)\n            results.append({\n                \"filename\": pdf_file.name,\n                \"success\": False,\n                \"markdown\": \"\",\n                \"path_taken\": \"Failed (exception)\",\n                \"total_tokens\": 0,\n                \"error\": error_msg,\n                \"diagnostics\": {\"error\": error_msg}\n            })\n\n    # Summary\n    successful = sum(1 for r in results if r[\"success\"])\n    print(f\"\\n{'='*60}\")\n    print(f\"BATCH SUMMARY\")\n    print(f\"{'='*60}\")\n    print(f\"Processed: {len(pdf_files)} files\")\n    print(f\"Successful: {successful}\")\n    print(f\"Failed: {len(pdf_files) - successful}\")\n    print(f\"Total tokens: {total_tokens:,}\")\n    print(f\"Output directory: {output_dir}\")\n    \n    if failed_files:\n        print(f\"\\nFailed files:\")\n        for fname in failed_files:\n            print(f\"  - {fname}\")\n\n    return results\n\n\n# Example usage (uncomment to run):\n# batch_results = batch_process(\"/content/pdfs\", \"/content/markdown_output\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell8_header"
   },
   "source": [
    "## Cell 8: Diagnostics & Debugging (Optional)\n",
    "\n",
    "Use this cell to inspect detailed diagnostics from the last run."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Display full diagnostics from last run\n",
    "if 'result' in dir() and result is not None:\n",
    "    print(\"Full Diagnostics:\")\n",
    "    print(json.dumps(result[\"diagnostics\"], indent=2))\n",
    "else:\n",
    "    print(\"No results available. Run Cell 6 first.\")\n",
    ""
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oT6Bx4a5mNQQ",
    "outputId": "b3f256c1-810d-4103-ef3b-3429a82f6cbc"
   },
   "execution_count": 43,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Full Diagnostics:\n",
      "{\n",
      "  \"markitdown_extraction\": {\n",
      "    \"success\": true,\n",
      "    \"char_count\": 25632,\n",
      "    \"error\": null\n",
      "  },\n",
      "  \"heuristic_check\": {\n",
      "    \"passed\": false,\n",
      "    \"issues\": [\n",
      "      \"No headers found in long document (3688 words)\"\n",
      "    ],\n",
      "    \"metrics\": {\n",
      "      \"word_count\": 3688,\n",
      "      \"words_per_page\": 461.0,\n",
      "      \"whitespace_ratio\": 0.16604244694132334,\n",
      "      \"header_count\": 0,\n",
      "      \"artifact_count\": 4,\n",
      "      \"table_line_count\": 0\n",
      "    }\n",
      "  },\n",
      "  \"claude_quality_check\": null,\n",
      "  \"final_conversion\": {\n",
      "    \"method\": \"claude_vision_fallback\",\n",
      "    \"reason\": \"heuristic_check_failed\",\n",
      "    \"tokens\": 49046\n",
      "  }\n",
      "}\n"
     ]
    }
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}